
## Adapter 代码实现

- 在每个 GPTBlock 里插入 Adapter（瓶颈 MLP + 残差）
- 冻结原模型参数，只训练 Adapter（以及你想训练的 head / embedding 可选）
- 优化器只传入 requires_grad=True 的参数

我用的是最经典的 Houlsby 风格（Attention 后一个 Adapter + FFN 后一个 Adapter）



#### 1 新增 Adapter 模块


```py
class Adapter(nn.Module):
    """
    Houlsby Adapter: bottleneck MLP + residual
    h -> down -> act -> up -> dropout -> +h
    """
    def __init__(self, d_model, bottleneck=64, dropout=0.1):
        super().__init__()
        self.down = nn.Linear(d_model, bottleneck)
        self.act = nn.GELU()
        self.up = nn.Linear(bottleneck, d_model)
        self.dropout = nn.Dropout(dropout)

        # 常见做法：把 up 初始化为 0，让初始时 adapter≈0，不破坏原模型
        nn.init.zeros_(self.up.weight)
        nn.init.zeros_(self.up.bias)

    def forward(self, x):
        return x + self.dropout(self.up(self.act(self.down(x))))
```

#### 2 改 GPTBlock：在 Attention/FFN 后插 Adapter

把原来的 GPTBlock 替换成下面这个版本（保留原来的 LayerNorm/残差逻辑，只是在每个子层输出后加 Adapter）：

```py
class GPTBlock(nn.Module):
    def __init__(self, d_model, num_heads, dff, dropout, adapter_bottleneck=64):
        super().__init__()

        self.mha_block = MultiheadAttention(d_model=d_model, num_heads=num_heads, dropout=dropout)
        self.ffn_block = FeedForward(d_model=d_model, dff=dff, dropout=dropout)

        # 两个 Adapter：一个给 Attention 输出，一个给 FFN 输出
        self.attn_adapter = Adapter(d_model=d_model, bottleneck=adapter_bottleneck, dropout=dropout)
        self.ffn_adapter  = Adapter(d_model=d_model, bottleneck=adapter_bottleneck, dropout=dropout)

        self.layernorm1 = LayerNorm(normalized_shape=d_model)
        self.layernorm2 = LayerNorm(normalized_shape=d_model)

    def forward(self, x, key_padding_mask):
        # 1) Causal Attention + Adapter + Residual + LN
        x_copy = x
        x = self.mha_block(q=x, k=x, v=x, key_padding_mask=key_padding_mask)
        x = self.attn_adapter(x)              # <--- 插入 Adapter
        x = self.layernorm1(x_copy + x)

        # 2) FFN + Adapter + Residual + LN
        x_copy = x
        x = self.ffn_block(x)
        x = self.ffn_adapter(x)               # <--- 插入 Adapter
        x = self.layernorm2(x_copy + x)

        return x
```

#### 3 改 GPT：把 adapter_bottleneck 透传进去（可选参数）

把 `GPT.__init__` 里创建 block 的地方改成这样：
```py
class GPT(nn.Module):
    def __init__(self, vocab_size, d_model, max_length, num_heads, dff, dropout, N, adapter_bottleneck=64):
        super().__init__()

        self.d_model = d_model

        self.emb_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)
        self.pos_layer = nn.Embedding(num_embeddings=max_length, embedding_dim=d_model)

        self.decoder_blocks = nn.ModuleList(
            [
                GPTBlock(d_model=d_model, num_heads=num_heads, dff=dff, dropout=dropout,
                         adapter_bottleneck=adapter_bottleneck)
                for _ in range(N)
            ]
        )

        self.cls = nn.Linear(in_features=d_model, out_features=vocab_size)
        self.pos_range = torch.arange(max_length).view(1, -1)

    def forward(self, x, key_padding_mask=None):
        emb = self.emb_layer(x)
        pos = self.pos_layer(self.pos_range[:, :x.size(1)])
        x = emb + pos

        for block in self.decoder_blocks:
            x = block(x=x, key_padding_mask=key_padding_mask)

        return self.cls(x)
```

#### 4 冻结主干参数，只训练 Adapter（+ 可选 cls）

在 `__main__` 里 model = GPT(...) 后面，加这段：
```py
# 1) 先冻结全部参数
for p in model.parameters():
    p.requires_grad = False

# 2) 解冻 Adapter 参数（以及你想训练的 head）
for name, p in model.named_parameters():
    if "attn_adapter" in name or "ffn_adapter" in name:
        p.requires_grad = True

# 可选：通常语言建模任务 cls head 也一起训会更稳
for name, p in model.named_parameters():
    if name.startswith("cls."):
        p.requires_grad = True

# （可选）如果你希望 embedding 也训：打开下面两段
# for name, p in model.named_parameters():
#     if name.startswith("emb_layer.") or name.startswith("pos_layer."):
#         p.requires_grad = True

# 3) 优化器只传入可训练参数
trainable_params = [p for p in model.parameters() if p.requires_grad]
optim = torch.optim.Adam(trainable_params, lr=0.001)

print("Trainable params:", sum(p.numel() for p in trainable_params))
print("Total params:", sum(p.numel() for p in model.parameters()))
```

到这里就完成了“全量训练 → Adapter 微调”的转换。

#### 你会立刻遇到的一个“现实问题”（别踩坑）

现在是从零随机初始化训练这套 GPT。
而 Adapter 微调的前提通常是：有一个已经预训练好的 base model，冻结它，再用 Adapter 去适配新任务。

所以如果你“从零训练 + 冻结主干”，模型可能学不动（因为主干是随机的，冻结后根本没能力）。你有两种合理玩法：
- 玩法 A（最标准）：先全量训练/预训练出 base（或加载已有权重） → 冻结 → 训练 Adapter
- 玩法 B（折中）：训练初期先全量训练若干 epoch（warmup）→ 再冻结主干，仅训练 Adapter（继续收敛）