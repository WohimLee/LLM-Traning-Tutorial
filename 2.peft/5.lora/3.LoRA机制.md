## 第3课：LoRA机制－参数冻结与增量学习核心
>课程目标
- 理解 LoRA 微调机制的核心思想与数学表达式
- 掌握 LoRA 中重要超参数 $\alpha$ 、rank 的设定原则
- 实践：在 LLaMA 2 7B 上微调 SST－2 数据集

### 1 LoRA 的基本原理
#### 1.1 权重冻结与扰动表达式
- 微调方式：冻结原始参数 $W_0$ ，仅学习扰动：$W=W_0+\Delta W$
- `LoRA` 构造：$\Delta W=\alpha \cdot A B^T \quad$ 其中 $A \in \mathbb{R}^{d \times r}, B \in \mathbb{R}^{k \times r}$

#### 1.2 超参数解释
- $r$ ：`秩（rank）`，控制插入的表示能力（通常 `4`、`8`、`16`）
- $\alpha$ ：缩放系数，常设置为 $\alpha=16,32$ 等
- `dropout`：可选，增强泛化能力（推荐 $0.05 \sim 0.1$ ）

#### 1.3 存储与部署优势
- 保存 `A` 和 `B` 而非整个权重矩阵，形成 `Adapter` 权重文件（几 MB）
- 可与多个任务形成并存 `Adapter`，支持任务间快速切换（`Merge－on－the－fly`）

