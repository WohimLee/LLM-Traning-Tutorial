## 第2课: 数学基础 - LoRA的线性代数核心

>课程目标

- 掌握 `LoRA` 所依赖的数学基础：`低秩分解`、`SVD`、`Kronecker积`
- 分析 `Transformer` 中适合插入 `LoRA` 的位置
- 通过 `NumPy` 编程加深对低秩扰动的理解

### 1 低秩分解回顾
#### 1.1 低秩矩阵定义
- 给定矩阵 $W \in \mathbb{R}^{d \times k}$ ，若存在两个矩阵 $A \in \mathbb{R}^{d \times r}, B \in \mathbb{R}^{k \times r}$ ，使得： $W \approx A B^T \quad(r \ll \min (d, k))$ 则称 $W$ 可低秩逼近。

#### 1.2 奇异值分解（SVD）
- 任意矩阵都可分解为：$W=U \Sigma V^T$
    - $U \in \mathbb{R}^{d \times d}, V \in \mathbb{R}^{k \times k}$ ：正交矩阵
    - $\Sigma \in \mathbb{R}^{d \times k}$ ：奇异值对角矩阵
- 低秩逼近：保留前 $r$ 个奇异值形成 $\hat{W}_r$ ，最优意义下逼近 $W$

#### 1.3 Kronecker积简述（可选）
- 用于扩展小矩阵参数表示能力
- 虽非 `LoRA` 主流实现，但部分工作（如 `K－LoRA`）使用它增强表示

### 2 LoRA 插入点分析

- `Transformer` 结构中的关键模块：
    - `Self-Attention`: `Query(Q)`, `Key(K)`, `Value(V)`, `Output`
    - `MLP` 层中的全连接子模块（`FFN`）
- `LoRA` 常见插入位置：
    - `Attention` 层中的 `Q` 和 `V` 权重矩阵（效果较显著）
    - `FFN` 层（增强模型泛化能力）
- 插入原则：
    - 尽可能覆盖对任务重要的子空间变换
    - 影响下游任务性能但不会破坏整体稳定性


### 3 NumPy 复现 Rank‑1 LoRA
```py
import numpy as np

# 模拟原始全量权重矩阵（例如 d=4, k=3）
W = np.random.randn(4, 3)

# LoRA 构造：rank = 1
rank = 1
A = np.random.randn(4, rank)
B = np.random.randn(3, rank)

delta_W = A @ B.T
W_lora = W + delta_W

# 打印结果
print("原始 W:\n", W)
print("扰动 delta_W (rank-1):\n", delta_W)
print("融合后 W_lora:\n", W_lora)
```

分析：

- 通过 `delta_W` 的引入，模型在不改变原始 `W` 的情况下学习任务特定变化
- 只需存储/更新 `A` 和 `B`（共 `r*(d+k)` 个参数），大幅降低训练开销
