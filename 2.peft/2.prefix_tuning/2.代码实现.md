## 代码实现

Prefix-Tuning 的版本（每层 attention 注入可训练 prefix K/V，冻结原模型参数）。

- 新增 PrefixKV（每层一组 prefix K/V 参数）
- 改 MultiheadAttention.forward(...) 支持拼接 prefix
- 正确处理 causal mask + padding mask（prefix 永远可见；真实 token 只能看 prefix + 自己之前）
- 训练时：冻结 base，只训练 prefix（+可选 cls head）

重要提醒：Prefix-tuning 正常是“在已预训练 base上微调”。如果你从零随机初始化再冻结 base，会学不动。

####  1 新增：PrefixKV 模块（每个 block 一套）

放在 FeedForward 后面、GPTBlock 前面：

```py
class PrefixKV(nn.Module):
    """
    Per-layer prefix for attention KV.
    Stores trainable prefix K/V in head space: (num_heads, prefix_len, d_k)
    """
    def __init__(self, num_heads, d_k, prefix_len):
        super().__init__()
        self.prefix_len = prefix_len
        self.num_heads = num_heads
        self.d_k = d_k

        # (H, P, d_k)
        self.prefix_k = nn.Parameter(torch.randn(num_heads, prefix_len, d_k) * 0.02)
        self.prefix_v = nn.Parameter(torch.randn(num_heads, prefix_len, d_k) * 0.02)

    def forward(self, batch_size, device=None, dtype=None):
        pk = self.prefix_k.unsqueeze(0).expand(batch_size, -1, -1, -1)  # (B,H,P,d_k)
        pv = self.prefix_v.unsqueeze(0).expand(batch_size, -1, -1, -1)
        if device is not None:
            pk = pk.to(device)
            pv = pv.to(device)
        if dtype is not None:
            pk = pk.to(dtype)
            pv = pv.to(dtype)
        return pk, pv
```

#### 2 改 MultiheadAttention：拼 prefix + 新 mask

把原来的 `MultiheadAttention.forward` 整个替换成下面这个（接口多了 `prefix_k/prefix_v`）：

```py
class MultiheadAttention(nn.Module):
    def __init__(self, d_model, num_heads, dropout):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        assert d_model % num_heads == 0
        self.d_k = d_model // num_heads

        self.q_proj = nn.Linear(d_model, d_model)
        self.k_proj = nn.Linear(d_model, d_model)
        self.v_proj = nn.Linear(d_model, d_model)
        self.o_proj = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, q, k, v, key_padding_mask=None, prefix_k=None, prefix_v=None):
        """
        q,k,v: (B,T,D)
        key_padding_mask: (B,T)  True=有效token, False=pad
        prefix_k/prefix_v: (B,H,P,d_k)
        """
        B, Tq, _ = q.size()
        Tk = k.size(1)

        Q = self.q_proj(q).view(B, Tq, self.num_heads, self.d_k).transpose(1, 2)  # (B,H,Tq,d_k)
        K = self.k_proj(k).view(B, Tk, self.num_heads, self.d_k).transpose(1, 2)  # (B,H,Tk,d_k)
        V = self.v_proj(v).view(B, Tk, self.num_heads, self.d_k).transpose(1, 2)  # (B,H,Tk,d_k)

        P = 0
        if prefix_k is not None and prefix_v is not None:
            P = prefix_k.size(2)
            # concat on seq_len dim
            K = torch.cat([prefix_k, K], dim=2)  # (B,H,P+Tk,d_k)
            V = torch.cat([prefix_v, V], dim=2)

        Tk_total = P + Tk

        logits = (Q @ K.transpose(-1, -2)) / math.sqrt(self.d_k)  # (B,H,Tq,Tk_total)

        # ---- build allowed mask: prefix always visible; real tokens causal + padding ----
        device = logits.device
        # prefix allowed: (1,1,Tq,P) all True
        prefix_allow = torch.ones(1, 1, Tq, P, device=device, dtype=torch.bool) if P > 0 else None

        # causal for real part: (Tq,Tk) -> (1,1,Tq,Tk)
        causal = torch.tril(torch.ones(Tq, Tk, device=device, dtype=torch.bool)).unsqueeze(0).unsqueeze(0)

        if key_padding_mask is None:
            key_padding_mask = torch.ones(B, Tk, device=device, dtype=torch.bool)
        else:
            key_padding_mask = key_padding_mask.to(device=device, dtype=torch.bool)

        # key valid for real keys: (B,1,1,Tk)
        k_valid = key_padding_mask.unsqueeze(1).unsqueeze(1)

        # query valid（可选，但更干净）：(B,1,Tq,1)
        q_valid = key_padding_mask[:, :Tq].unsqueeze(1).unsqueeze(-1) if key_padding_mask.size(1) >= Tq else None

        real_allow = causal & k_valid  # (B,1,Tq,Tk)

        if P > 0:
            allow = torch.cat([prefix_allow.expand(B, -1, -1, -1), real_allow], dim=-1)  # (B,1,Tq,P+Tk)
        else:
            allow = real_allow

        if q_valid is not None:
            allow = allow & q_valid  # pad query positions全部mask掉

        logits = logits.masked_fill(~allow, -1e9)

        scores = torch.softmax(logits, dim=-1)
        out = scores @ V  # (B,H,Tq,d_k)

        out = out.transpose(1, 2).contiguous().view(B, Tq, self.d_model)
        return self.dropout(self.o_proj(out))
```

#### 3 改 GPTBlock：每层加 PrefixKV，并传给 attention

把你原 GPTBlock 替换成：

```py
class GPTBlock(nn.Module):
    def __init__(self, d_model, num_heads, dff, dropout, prefix_len=16):
        super().__init__()
        self.mha_block = MultiheadAttention(d_model=d_model, num_heads=num_heads, dropout=dropout)
        self.ffn_block = FeedForward(d_model=d_model, dff=dff, dropout=dropout)

        # 每层一套 prefix K/V（在 head-space）
        d_k = d_model // num_heads
        self.prefix = PrefixKV(num_heads=num_heads, d_k=d_k, prefix_len=prefix_len)

        self.layernorm1 = LayerNorm(normalized_shape=d_model)
        self.layernorm2 = LayerNorm(normalized_shape=d_model)

    def forward(self, x, key_padding_mask):
        B = x.size(0)
        pk, pv = self.prefix(batch_size=B, device=x.device, dtype=x.dtype)

        # Attention + Residual + LN
        x_copy = x
        x = self.mha_block(
            q=x, k=x, v=x,
            key_padding_mask=key_padding_mask,
            prefix_k=pk, prefix_v=pv
        )
        x = self.layernorm1(x_copy + x)

        # FFN + Residual + LN
        x_copy = x
        x = self.ffn_block(x=x)
        x = self.layernorm2(x_copy + x)
        return x
```

#### 4 改 GPT：创建 block 时传 prefix_len

把 `GPT.__init__` 里创建 blocks 的地方改为：

```py
class GPT(nn.Module):
    def __init__(self, vocab_size, d_model, max_length, num_heads, dff, dropout, N, prefix_len=16):
        super().__init__()
        self.d_model = d_model
        self.emb_layer = nn.Embedding(vocab_size, d_model)
        self.pos_layer = nn.Embedding(max_length, d_model)

        self.decoder_blocks = nn.ModuleList(
            [GPTBlock(d_model, num_heads, dff, dropout, prefix_len=prefix_len) for _ in range(N)]
        )

        self.cls = nn.Linear(d_model, vocab_size)
        self.pos_range = torch.arange(max_length).view(1, -1)

    def forward(self, x, key_padding_mask=None):
        emb = self.emb_layer(x)
        pos = self.pos_layer(self.pos_range[:, :x.size(1)].to(x.device))
        x = emb + pos

        for block in self.decoder_blocks:
            x = block(x=x, key_padding_mask=key_padding_mask)

        return self.cls(x)
```

#### 5 训练：冻结 base，只训练 prefix（+可选 cls）

在 `model = GPT(...)` 后面加：

```py
prefix_len = 16
model = GPT(vocab_size, d_model, max_length, num_heads, dff, dropout, N, prefix_len=prefix_len)

# 1) Freeze all
for p in model.parameters():
    p.requires_grad = False

# 2) Unfreeze prefix params
for name, p in model.named_parameters():
    if "prefix." in name:
        p.requires_grad = True

# 3) 可选：也训练 cls head（常见更稳）
for name, p in model.named_parameters():
    if name.startswith("cls."):
        p.requires_grad = True

trainable = [p for p in model.parameters() if p.requires_grad]
optim = torch.optim.Adam(trainable, lr=1e-3)

print("Trainable params:", sum(p.numel() for p in trainable))
print("Total params:", sum(p.numel() for p in model.parameters()))
```
