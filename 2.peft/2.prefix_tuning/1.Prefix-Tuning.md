## Prefix-Tuning

### 一、Prefix Tuning 是什么？（一句话）

Prefix Tuning = 在 Transformer 的每一层 Attention 中，给 Key / Value 拼接一段可训练的“前缀向量”，冻结原模型参数。

核心不是“改输入”，而是 改注意力的上下文。

### 二、Prefix Tuning 在干什么？（直觉）

原始 self-attention：
```
Q = XWq
K = XWk
V = XWv
Attn(Q,K,V)
```

Prefix Tuning 后：
```
K' = concat(Pk, K)
V' = concat(Pv, V)
Attn(Q, K', V')
```

其中：
- Pk, Pv 是可训练参数
- 每一层都有自己的一套 prefix
- 原模型权重 完全冻结

### 三、Prefix 的参数形状（关键细节）

假设：
- 层数 = L
- 头数 = H
- 每头维度 = d_k
- prefix 长度 = m

那么一层 prefix 参数是：
```
Pk ∈ R[m, H, d_k]
Pv ∈ R[m, H, d_k]
```

总参数量：
```
2 × L × m × H × d_k
```

👉 这也是为什么：
- prefix 长度 m 不能太大
- 层数 L 越多，prefix 越贵

### 四、Prefix Tuning 的训练方式
#### 1️⃣ 原论文（Li & Liang, 2021）的做法

不是直接训练 Pk/Pv，而是：
```
z → MLP → Pk/Pv
```

好处：

- 低维 latent 更稳定
- 可控参数规模

#### 2️⃣ 训练流程

- 冻结整个 Transformer
- 只更新 prefix 参数（或 prefix-MLP）
- loss 和全量微调一模一样

### 五、Prefix Tuning vs Prompt Tuning（本质区别）
| 对比项   | Prompt Tuning | Prefix Tuning |
| ----- | ------------- | ------------- |
| 插入位置  | 输入 embedding  | Attention KV  |
| 影响深度  | 仅第一层          | 每一层           |
| 表达能力  | 弱             | 强             |
| 大模型效果 | 差             | 好             |
| 工程复杂度 | 低             | 高             |


一句话：Prompt 只影响“你看到了什么”，Prefix 影响“你如何注意”。

### 六、Prefix Tuning vs P-Tuning v2（关系）

**P-Tuning v2 = Prefix Tuning 的工程化 + 统一实现**

P-Tuning v2 做了三件关键事：

- 把 prefix 统一到多种模型（Encoder / Decoder / Encoder-Decoder）
- 支持深层 prefix
- 工程稳定性大幅提升

👉 理论上你可以把 P-Tuning v2 看成“更成熟的 Prefix Tuning”

### 七、Prefix Tuning vs Adapter
| 维度     | Prefix Tuning | Adapter           |
| ------ | ------------- | ----------------- |
| 注入方式   | Attention KV  | FFN / Attention 后 |
| 是否改结构  | 否             | 是                 |
| 推理额外算力 | 少             | 多                 |
| 参数量    | 小             | 中                 |
| 多任务切换  | 好             | 极好                |


Adapter 改“变换”，Prefix 改“注意力上下文”。

### 八、Prefix Tuning vs LoRA（为什么被压制）
| 维度     | Prefix       | LoRA      |
| ------ | ------------ | --------- |
| 插入位置   | Attention KV | Linear 权重 |
| 推理开销   | 有            | 无（可合并）    |
| 工程友好   | 一般           | 极好        |
| LLM 主流 | 否            | 是         |


👉 Prefix 没输在效果，输在工程与生态。

### 九、Prefix Tuning 的优缺点总结
#### ✅ 优点

- 冻结权重，安全
- 参数效率高
- 大模型友好（比 Prompt）

#### ❌ 缺点

- 工程实现复杂
- 推理时 attention 序列变长
- 不易与 FlashAttention 等融合

### 十、什么时候你还该用 Prefix Tuning？
#### 适合场景

- 权重必须冻结（合规 / 安全）
- 多任务 prefix 热切换
- 学术研究（attention 可控性）

#### 不推荐

- 工业 SFT
- 推理延迟敏感
- 需要生态支持

### 十一、一个非常重要的“认知点”

**Prefix Tuning 是第一个证明“大模型可以靠极少参数适配任务”的方法。**

它直接催生了：
- P-Tuning v2
- 后来的 LoRA 思路（低秩 ≈ 小子空间）

### 十二、一句话终极总结

Prefix Tuning 不是“过时”，而是“被更好工程化的方法替代”。