## 大模型做微调后评测

建议把评测分成三层：**能力是否提升**、**是否产生副作用（遗忘/安全/风格漂移）**、**是否真的能在业务场景里稳定工作**。下面是一套可以直接落地的评测框架（偏通用，不依赖具体微调方式：SFT / DPO / LoRA / 全参都适用）。

#### 1) 先定“评测目标”与对照组

**必须做对照**，否则看不出微调带来的真实收益：

* **Base**：原始 Qwen3-8B（同一版本、同一推理配置）
* **FT**：微调后的模型
* （可选）**FT+RAG/工具**：如果线上会用检索/工具调用，也要单独评测这一套

评测时固定这些变量，否则结果不可比：

* 解码参数：temperature/top_p/max_tokens/stop
* system prompt / 模板
* 是否使用思维链（建议评测时不要求显式思维链，统一按最终答案判）
* 版本与量化方式（fp16/int8/int4 都会影响）



#### 2) 三类数据集：离线、回归、线上

##### A. 离线能力评测（通用基准）

用于回答：**变强了多少？强在哪？**
常见方向（可按任务选择）：

* **通用推理/数学/代码/知识**：MMLU、CMMLU、GSM8K、HumanEval/MBPP 等
* **中文能力**：C-Eval、CMMLU、GAOKAO/作文类集合等
* **对话与指令遵循**：AlpacaEval/MT-Bench 风格（中文也可自建）

> 这里最关键不是“跑哪些榜”，而是：**跑同一套题、同一套脚本、同一套解码**，比较 Base vs FT 的差值。

##### B. 回归评测（防止“变强但变坏”）

用于回答：**有没有遗忘、幻觉变多、输出变啰嗦/变短、拒答率异常**。

至少要测：

* **遗忘/能力退化**：在不想改变的能力上抽一套固定题（比如通用问答/写作/代码）
* **安全与合规**：敏感问题拒答是否符合预期（尤其做过对齐类微调）
* **风格一致性**：格式要求、口吻、长度、结构化输出稳定性（JSON/表格等）
* **鲁棒性**：错别字、口语、长上下文、多轮对话、指令冲突

##### C. 线上/准线上评测（最重要）

用于回答：**它在真实流量下是否稳定更好**。

* 用真实用户 query（脱敏后）抽样
* A/B：Base vs FT（或灰度）
* 指标：成功率、人工满意度、工单率、拒答率、平均轮数、平均token、延迟

---

#### 3) 指标体系：任务不同，指标不同

##### (1) 有标准答案的任务（分类/抽取/检索/结构化）

* Accuracy / F1 / EM（Exact Match）
* JSON schema 通过率（强烈建议加）
* 业务指标：召回率、错误类型占比（漏抽/错抽/格式错）

##### (2) 生成式任务（总结/写作/对话/客服）

自动指标只能做辅助：

* ROUGE / BLEU（仅作趋势）
* BERTScore（可用但也有限）
  更可靠的是：
* **LLM-as-a-Judge**（模型裁判）
* **人工评测**（抽样 + 标注规范）

##### (3) LLM-as-a-Judge 怎么做才靠谱

* 用“裁判模型”固定版本、固定prompt
* 让裁判输出结构化打分：正确性/完整性/有害性/格式遵循/引用是否充分
* 最好做：**pairwise**（A vs B 哪个更好）而不是单点评分
* 抽查裁判一致性：同一题重复评一次，看波动

---

#### 4) 针对微调类型的特别检查

##### 如果做的是 SFT（监督微调）

重点看：

* 指令遵循提升是否显著
* 是否过拟合某种模板（输出变机械）
* 幻觉/编造是否增加（尤其是知识问答）

##### 如果做的是 DPO / PPO / RLHF 风格

重点看：

* 拒答率是否异常升高（“过度对齐”）
* 解释性是否变差（答非所问）
* 在边界问题上是否更稳

##### 如果用 LoRA/QLoRA

重点看：

* 不同量化/不同推理后端下是否一致（int4 有时会掉格式稳定性）
* 合并权重 vs 运行时挂载 LoRA 的差异



#### 5) 一套“最小可用”的评测落地流程（建议直接照做）

1. **整理评测集**

   * 业务集：200～1000 条（越贴近线上越好）
   * 回归集：200 条（不希望退化的能力）
   * （可选）通用基准：按需选

2. **批量推理出结果**

   * Base 与 FT 各跑一遍
   * 输出保存为 JSONL：`{id, prompt, output, model, params}`

3. **自动打分**

   * 有标准答案：脚本算 EM/F1/Schema pass
   * 生成类：裁判模型做 pairwise + 维度打分

4. **人工抽检**

   * 按错误类型分桶（格式错/事实错/拒答/跑题/啰嗦）
   * 每桶抽 20～50 条，看主要退化点

5. **产出报告**

   * 总体胜率（FT>Base 的比例）
   * 关键指标变化（成功率、拒答率、格式通过率）
   * Top 失败案例（带 prompt、输出、原因、修复建议）



#### 6) Qwen3-8B 评测时容易踩的坑

* **模板不一致**：Qwen 系列对 chat template 很敏感，Base 和 FT 必须用同一套模板
* **解码参数导致“看起来变强”**：温度不同会显著影响主观质量
* **只看平均分不看失败类型**：线上最致命的是格式失败、拒答异常、幻觉增多
* **评测数据泄漏**：如果评测题混进了训练集，会虚高（建议做去重：按 n-gram 或 embedding 近邻）

