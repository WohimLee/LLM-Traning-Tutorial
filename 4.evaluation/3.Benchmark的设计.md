
## Benchmark 的设计
**90% 的评测失真，根源都不在模型，而在 benchmark 设计本身**。
我会按 **“从直觉 → 工程 → 可落地”** 的方式给讲清楚：**什么是 benchmark 设计、为什么重要、以及该怎么自己设计一个靠谱的 benchmark（尤其针对 Qwen3-8B 微调）**。



#### 一句话定义

> **Benchmark 设计** =
> **用一组“可重复、可比较、可解释”的测试任务，来衡量模型在某种能力或业务目标上的表现。**

它回答的不是“模型强不强”，而是：

> 👉 **模型在「关心的能力」上，是否比对照模型更好，而且这个结论是否可信？**



### 一、为什么“跑 benchmark ≠ benchmark 设计”

很多人误以为：

> “我跑了 CMMLU / C-Eval / OpenCompass，所以 benchmark 就有了”

❌ **错**。
只是**使用了别人设计的 benchmark**，但**评测目标可能完全不同**。

举例：

* 微调的是 **客服对话稳定性**
* 跑的是 **多选题知识问答**
* 得分提升 ≠ 客服质量提升

👉 **benchmark 设计的核心是“对齐目标”**。



### 二、benchmark 设计的 4 个核心要素（记住这四个词）

#### ① 能力定义（Capability）

到底想测什么？

❌ 错误说法：

> “测模型综合能力”

✅ 正确说法：

* 指令遵循（多约束）
* 事实一致性（是否编造）
* 结构化输出稳定性（JSON）
* 领域知识召回
* 拒答边界是否合理

👉 **一个 benchmark 只能测 1–2 个核心能力**


#### ② 任务形式（Task Format）

同一个能力，不同任务形式，结论可能完全相反。

| 能力   | 好的任务形式 | 坏的任务形式    |
| ---- | ------ | --------- |
| 指令遵循 | 多条件生成  | 单问单答      |
| 抽取   | 真实噪声文本 | 人工整理文本    |
| 对话   | 多轮上下文  | 单轮 prompt |
| 稳定性  | 格式校验   | 主观“看着还行”  |

👉 **任务形式比模型参数更重要**



#### ③ 评判方式（Metric & Judge）

这是最容易被忽视、但最致命的一环。

##### 三种判分方式

1. **确定性指标**（最靠谱）

   * Accuracy / F1 / EM
   * Schema 校验
2. **弱自动指标**

   * ROUGE / BLEU（仅看趋势）
3. **裁判模型（LLM-as-a-Judge）**

   * Pairwise > 打分
   * 维度拆分 > 单一分数

⚠️ **不要混用指标却不解释它们测的是什么**



#### ④ 对照与控制（Control）

没有对照，就没有 benchmark。

必须控制：

* Base vs FT
* 解码参数
* Prompt 模板
* 评测数据泄漏

否则测到的是**噪声**，不是能力。



### 三、一个“好 benchmark”长什么样？

#### ✅ 好 benchmark 的 7 个特征

1. 和业务/目标强相关
2. 微调前后差异可被放大
3. 对模型作弊不友好
4. 可重复（今天跑 vs 明天跑）
5. 可解释（知道为什么错）
6. 覆盖失败边界
7. 成本可控（能长期跑）



### 四、一个反例（非常常见）

> “我微调了一个客服模型，用 OpenCompass 跑 CMMLU，涨了 3 分”

问题在哪？

* CMMLU 测的是学术知识
* 客服最重要的是 **稳定性 / 拒答 / 多轮理解**
* benchmark 与目标**不对齐**

👉 结论极可能是**虚假的成功**



### 五、如何为 Qwen3-8B 微调设计一个 benchmark（实战模板）



#### Step 1：定义能力（最多 3 个）

例：

* 指令遵循（多约束）
* 幻觉控制
* JSON 输出稳定性



#### Step 2：设计任务

```json
{
  "id": "cs_0421",
  "context": "用户投诉物流延误……",
  "instruction": "用不超过100字回复，必须包含道歉和解决方案，不要承诺赔偿",
  "output_format": "json",
  "schema": {
    "apology": "string",
    "solution": "string"
  }
}
```



#### Step 3：评测指标

* JSON Schema 通过率（硬指标）
* LLM Judge：是否违反指令（是/否）
* 人工抽检：幻觉 / 不当承诺


#### Step 4：对照

* Qwen3-8B Base
* Qwen3-8B FT
* 同一 prompt / 同一参数



#### Step 5：报告结论

不是：

> “准确率 82%”

而是：

> “在**多约束指令 + 结构化输出**任务中，
> FT 模型的成功率从 **61% → 84%**，
> 主要提升来自 **格式错误显著减少**。”



### 六、benchmark ≠ 数据集（这是进阶理解）

* **数据集**：一堆题
* **benchmark**：
  数据集 + 任务定义 + 判分规则 + 对照方式 + 报告解读

👉 benchmark 是一个 **实验设计**，不是一个文件夹。



### 七、给“判断 benchmark 好坏”的 3 个问题

以后看到任何评测，只问这 3 个问题：

1. **它测的能力，和我要的能力是一回事吗？**
2. **如果模型作弊，能不能轻易拿高分？**
3. **我能不能复现并解释每一次失败？**

答不上来 → benchmark 有问题。
