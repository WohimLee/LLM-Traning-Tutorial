## 从全量微调到LoRA

>课程目标

- 理解 LoRA 的研究背景与发展历程
- 掌握 LoRA 为何成为当前主流的参数高效微调技术
- 初步了解 LoRA 的设计理念与其带来的能效优势

### 1 发展脉络与技术演进

#### 1.1 全量微调（Full Fine-tuning）

- 定义：调整模型中全部参数，适应下游任务
- 成本：高显存、高训练时长、高计算能耗
- 缺陷：每个任务都需保存一份完整模型副本

#### 1.2 Adapter 方法

- 定义：在原始模型层间插入小型网络块，仅更新这些块参数
- 优点：大幅减少训练参数量
- 代表工作：AdapterHub、Compacter
- 局限：引入推理延迟；与主干参数结构绑定紧密

#### 1.3 LoRA（Low-Rank Adaptation）
- 提出时间：ICLR 2021，论文《LoRA: Low-Rank Adaptation of Large Language Models》

- 核心思想：冻结预训练模型，仅在参数矩阵上添加低秩扰动 $∆W = AB^T$

- 优点：
    - 高效训练：可只训练 `0.1%` 的参数
    - 插件式设计：适配不同规模与任务
    - 与量化等技术兼容（如QLoRA）
- 应用：广泛应用于LLaMA、ChatGLM、Baichuan等大语言模型微调中

### 2 绿色 AI 与训练成本分析


微调方式 | 可训综坆数量 |显存占用 | 能傃成本 | 櫵型复用性
---|---|---|---|---
全量微调  |100％ | 高 | 高 | 差（需多副本） 
Adapter |＜10％ | 中 | 中 |中
LoRA | ＜1％ | 低 | 低 | 强

- LoRA具有出色的能效比，适配边缘设备部署与持续迭代任务
- 支持低资源国家/地区的大模型研究工作

### 3 论文导读：《LoRA: Low-Rank Adaptation》

##### 作者信息

Edward J. Hu 等（微软、华盛顿大学）

##### 主要贡献

- 提出低秩权重扰动机制（$ΔW = AB^T$）代替全量权重更新
- 实证：`BERT / RoBERTa / GPT-2` 在多任务集上效果接近甚至优于全量微调
- 分析：理论上能保留模型预训练能力的同时提升参数效率

##### 关键技术点
- 插入点选择：主要在 `Attention` 层`（q,k,v）`与 `MLP` 层
- 超参数：
    - `rank r`：通常选为 `4` 或 `8`
    - `scaling α`：用于控制扰动幅度（如 `α=32`）

##### 结果概览：

多个 `NLP` 任务上，`LoRA` 微调仅用 `0.5%-1%` 参数就能达到 `95%-98%` 的全量效果

### 4 课堂讨论题

- `LoRA` 是否适用于图像、音频等模态？为何 `Transformer` 结构是关键？
- 相比 `Adapter`，`LoRA` 在部署与维护方面有何优势？
- 若训练数据频繁更新，哪种微调方案更适合持续学习？

### 5 本课小结

- `LoRA` 处在参数高效微调技术的核心地位
- 其提出解决了传统微调的成本与资源难题
- 下一课将探讨 `LoRA` 背后的数学基础：为何低秩近似能“接管”复杂网络

### 6 课后任务

- 阅读原论文前两节（引言 + 方法）
- 思考：`LoRA` 是否本质是一种压缩方法？你如何解释其泛化能力？