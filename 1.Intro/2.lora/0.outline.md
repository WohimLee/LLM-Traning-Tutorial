## 课程大纲
面向高校 / 企业培训的 “LoRA（Low‑Rank Adaptation）参数高效微调技术” 教学大纲示
### 课程概览

| 项目  | 说明                                                                                                                 |
| --- | ------------------------------------------------------------------------------------------------------------------ |
| 受众  | 具备 Python 与深度学习基础、已使用过 PyTorch/Hugging Face 的研究生、算法工程师或高校教师                                                        |
| 总学时 | 36 学时（12 周，每周 3 学时：2 h 讲授 + 1 h 实践/讨论）                                                                             |
| 目标  | 1) 掌握 LoRA/QLoRA 等 PEFT 核心原理 2) 能独立实现、调优并部署 LoRA 插件 3) 了解 RandLoRA、HydraLoRA 等前沿方向并评估其适用场景([arXiv][1], [arXiv][2]) |

[1]: https://arxiv.org/html/2407.11046v3?utm_source=chatgpt.com "A Survey on LoRA of Large Language Models - arXiv"
[2]: https://arxiv.org/html/2502.00987v2?utm_source=chatgpt.com "RandLoRA: Full-rank parameter-efficient fine-tuning of large models"

### 先修知识
- 线性代数：矩阵分解、切比雪夫不等式
- 深度学习：BP 与 Adam 优化、Transformer 架构
- 量化与剪枝入门（建议阅读 QLoRA 论文前两节）

### 课程结构与每周主题

| 周次 | 模块                      | 授课要点                                                | 实践 / 作业                                 |
| -- | ----------------------- | --------------------------------------------------- | --------------------------------------- |
| 1  | 引论                      | 发展脉络：从全量微调到 Adapter → LoRA；成本 & 绿色 AI ([Medium][1]) | 论文速读：原始 LoRA                            |
| 2  | 数学基础                    | 低秩分解、Kronecker & SVD 复习；LoRA 插入点分析                  | 用 NumPy 复现 rank‑1 LoRA                  |
| 3  | LoRA 机制                 | 权重冻结 + ΔW = AB<sup>T</sup>；α / r 超参数；增量存储           | 在  7B Llama 2 上做 SST‑2                  |
| 4  | 实现细节                    | PyTorch 实践：LoRALinear、Auto‑wrap；Grad‑Checkpoint     | 写一个支持可插拔 rank 的 LoRA 层                  |
| 5  | QLoRA                   | 4‑bit 量化 + LoRA；NF4 格式；Double Quant                 | 复现 QLoRA 微调并监控 VRAM([Databricks][2])    |
| 6  | 评估指标                    | Perplexity、Exact Match、LoRA merge 误差；能耗             | 设计一套对话评测脚本                              |
| 7  | 进阶：RandLoRA & HydraLoRA | 全秩随机旋转、路由 LoRA 机制；多任务路由开关([arXiv][3])               | 改写代码以支持多 LoRA 分支                        |
| 8  | LoRA‑Gen & Edge AI      | 云端参数生成、边缘融合框架；隐私与联邦 LoRA                            | 组队：在树莓派上部署聊天机器人([Quantum Zeitgeist][4]) |
| 9  | 多模态 LoRA                | Stable Diffusion、Vision Transformer 插件；LoRA‑SDXL 实例 | 训练一个风格化 LoRA 模型                         |
| 10 | 安全与鲁棒性                  | 对抗样本下的 LoRA；参数泄露风险                                  | 实验：LoRA 权重重定位攻击                         |
| 11 | 工程化                     | Checkpoint 合并、权重剪裁；ONNX/TensorRT 导出                 | 将多 LoRA 合并为单 fp16 模型                    |
| 12 | 期末项目                    | 学生项目答辩 & 代码审查                                       | 交付：技术报告 + Github Repo                   |

[1]: https://medium.com/foundation-models-deep-dive/parameter-efficient-fine-tuning-for-llms-lora-qlora-and-beyond-97e3714a1f8a?utm_source=chatgpt.com "Parameter-Efficient Fine-Tuning for LLMs: LoRA, QLoRA, and Beyond"
[2]: https://www.databricks.com/blog/efficient-fine-tuning-lora-guide-llms?utm_source=chatgpt.com "Efficient Fine-Tuning with LoRA for LLMs | Databricks Blog"
[3]: https://arxiv.org/html/2502.00987v2?utm_source=chatgpt.com "RandLoRA: Full-rank parameter-efficient fine-tuning of large models"
[4]: https://quantumzeitgeist.com/lora-gen-boosts-edge-ai-performance-with-cloud-based-parameter-generation/?utm_source=chatgpt.com "LoRA-Gen Boosts Edge AI Performance With Cloud-based ..."


### 教学资源
- 核心论文
    - Hu et al., LoRA: Low‑Rank Adaptation (2021)
    - Dettmers et al., QLoRA (2023)
    - Tian et al., HydraLoRA (2024)
    - Park et al., RandLoRA (Feb 2025)

- 最新综述与博文
    - Parameter‑Efficient Fine‑Tuning for LLMs: LoRA, QLoRA and Beyond (Jun 2025)
    - Efficient LLMs: A Survey (GitHub 2025)
- 工具链
    - Hugging Face peft & merge_adapter
    - bitsandbytes ≥ 0.43 量化库
    - Weights & Biases / TensorBoard for实验跟踪

### 评估方式

| 占比   | 任务                         |
| ---- | -------------------------- |
| 30 % | 每周编程作业（自动评分 + code review） |
| 20 % | 文献分享与课堂讨论                  |
| 50 % | 期末项目（报告 + 公开仓库 + 演示视频）     |
