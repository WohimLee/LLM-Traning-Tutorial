# 高效微调（Efficient Fine-tuning）
高效微调（Efficient Fine-tuning）是在保证模型性能的前提下，降低微调大语言模型（LLM）成本（时间、显存、计算资源等）的一系列方法。主要有以下几种常见的高效微调手段：

### 1 参数高效微调（PEFT, Parameter-Efficient Fine-Tuning）
只更新模型的一小部分参数，从而减少训练成本：

##### LoRA（Low-Rank Adaptation）

- 核心思想：在原始权重矩阵上添加可训练的低秩矩阵。
- 优点：显存占用小，效果好，训练快。
- 应用广泛，如在 LLaMA、GPT 等模型中常用。

##### Adapter

- 在每层Transformer中插入小的可训练模块（adapter层），冻结原模型。
- 参数增量小、结构清晰。
- 变种：Compacter, Houlsby Adapter, Parallel Adapter等。

##### Prefix Tuning
- 为每层 Transformer 添加可学习的“前缀向量”（prompt tokens），不更新模型参数。
- 适用于生成任务。

##### Prompt Tuning / P-Tuning / P-Tuning v2

- Prompt Tuning：用固定的 embedding 向量作为 prompt，直接训练这些向量。
- P-Tuning v2：将 prompt 向量送入深层 Transformer 中，兼顾效率与性能。

### 2 数据选择与样本高效化
##### 数据蒸馏 / 样本剪枝

- 使用代表性强的小规模数据子集代替全数据训练。
- 示例：Data selection based on influence functions 或 embedding similarity。

##### 少样本学习（Few-shot Learning）
- 精心设计提示词（prompt），让模型靠上下文学习任务。
- 结合PEFT方法效果更佳。

##### 对比学习 / Curriculum Learning
- 利用“由易到难”策略训练模型，提升学习效率。

### 3 优化训练方式
##### 混合精度训练（FP16/BF16）

- 使用低精度浮点数训练，提高速度和降低显存。
- 需要硬件支持（如NVIDIA A100，H100等）。

##### Gradient Checkpointing
- 降低显存占用，牺牲部分计算开销。

##### 冻结早期层
- 只微调模型的高层，冻结底层以节省资源。

### 4 工具与库支持
- HuggingFace 🤗 peft 库：支持LoRA、Prefix Tuning等方法，易于集成。
- DeepSpeed / FSDP：分布式训练和显存优化工具。
- LLaMA-Factory、QLoRA：针对高效微调的完整解决方案。