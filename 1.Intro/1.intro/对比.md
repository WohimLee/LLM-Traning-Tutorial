# 对比
## 1 Intro
>LoRA（Low-Rank Adaptation）
- 通过引入 `低秩矩阵` 对权重进行微调，只更新很少量的参数，避免全参数更新，节省显存和计算开销。典型应用是对大模型进行轻量级下游任务适配。

>QLoRA（Quantized LoRA）
- 在 LoRA 的基础上，将模型权重量化为 `4-bit`，冻结主模型，仅对插入的 LoRA 模块进行训练。进一步降低显存需求，支持在消费级硬件上训练大模型（如 65B 参数模型）。

>Prefix-Tuning
- 在注意力层前插入可训练的前缀向量，不修改原模型参数。相比 LoRA 更轻量，但表达能力有限，适用于少样本学习或控制生成风格。

》核心区别：

| 方法                | 原理简述                                      | 参数量   | 训练内存占用 | 支持模型类型  | 优点                    | 缺点                  |
| ----------------- | ----------------------------------------- | ----- | ------ | ------- | --------------------- | ------------------- |
| **LoRA**          | 冻结原模型，仅在注意力或FFN中引入低秩矩阵做残差更新               | 少（百万） | 低      | 编码器/解码器 | 训练快、部署方便，可组合多任务       | 不适合极小数据量或极小模型       |
| **QLoRA**         | 在量化模型上加 LoRA（4-bit量化 + NF4 + 双缓存策略）       | 极少    | 极低     | 大语言模型   | 超低显存，支持大模型训练          | 推理仍需加载整个量化模型        |
| **P-Tuning v2**   | 在模型输入前添加可训练的嵌入（soft prompt），用LSTM初始化优化表达力 | 中等    | 中等     | 编码器/解码器 | 表达能力强，适合小数据场景         | 实现稍复杂，推理时需拼接 prefix |
| **Prefix-Tuning** | 在注意力每层前加可训练的 KV 向量（prompt），引导注意力          | 中等    | 中等偏高   | 多层注意力结构 | 效果优于直接输入 prompt，适合多任务 | 每层加 prefix，推理时占用大些  |


## 2 操作细节
### 2.1 LoRA（Low－Rank Adaptation）
>操作层
- LoRA 通常插入在 Transformer 中的注意力层（Self－Attention）和 FFN（前渍网络）中的线性层 （Linear），尤其是 Wq，Wv，Wk，Wo，以及 FFN 中的 W1，W2 层。

>原理
- 将原始权重矩阵 $W \in \mathbb{R}^{d_{\text {out }} \times d_{\text {in }}}$ 表示为：

$$
W^{\prime}=W+\Delta W=W+A B, \quad A \in \mathbb{R}^{d_{\mathrm{out}} \times r}, B \in \mathbb{R}^{r \times d_{\mathrm{in}}}
$$


其中 $r \ll d$ ，只训练 A 和 B 两个低秩矩阵，其余部分冻结。

### 2.2 QLoRA（Quantized LoRA）
>操作层
- 和 LoRA 类似，主要针对 Transformer 中的 Linear 层，如 Attention 和 MLP 层的权重矩阵。

>关键区别
- 主模型被 `4－bit` 量化（如 NF4 格式）并冻结。
- 只在量化权重旁边插入可训绕的 LoRA 层。
- 使用 paged optimizers（如 bitsandbytes 的paged AdamW）优化显存使用。

>设计理念
- 在最大限度减少资源消耗的前提下，保持 LoRA 的表达能力，实现消费级硬件微调大模型。

### 2.3 Prefix－Tuning
>操作层
- 仅作用于 Transformer 的注意力层。它在每一层注意力机制前，注入一段可训综的前经向量，不俢改原有权重。

>实现方式
- 在每个 Transformer 层，添加一组 `prefix key` 和 `prefix value` 向量（可训综）；
- 在计算注意力时，将这些 prefix 向量与原序列亚接：

$$
K=\left[K_{\text {prefix }} ; K_{\text {input }}\right], \quad V=\left[V_{\text {prefix }} ; V_{\text {input }}\right]
$$
- Prefix 不随输入变化，因此非常轻量，可用于多任务或风格迁移

## 3 总结
>技术演进趋势解析：
- Prefix-Tuning（2021初） 是最早的参数高效微调技术之一，最轻量，但表达能力有限
- LoRA（2021年底） 大大增强了表达能力，已经被广泛用于各类 LLM 微调任务
- QLoRA（2023） 是真正“工程实用级”的突破，让普通用户也能低成本微调大模型，极大推动了社区民主化微调

📌 现实中组合使用趋势：

在 HuggingFace PEFT、LLaMA-Factory 等项目中，LoRA + QLoRA 已成为主流配置；

Prefix-Tuning 多用于少样本学习、多任务控制生成等轻量场景。

方法	|操作位置	|是否修改主模型	|是否量化	|参数量	|主要应用场景
:-|:-|:-|:-|:-|:-|
LoRA	|Attention + FFN 的 Linear 层	|否	|否	|中	|通用微调
QLoRA	|Attention + FFN 的 Linear 层	|否（主模型冻结）	|✅ 4bit	|中	|消费级硬件微调大型模型
Prefix-Tuning	|Attention 层（仅 KV 向量）	|否	|否	|少	|风格控制、快速任务适配