## 低秩分解（Low-Rank Decomposition）
低秩分解（Low-Rank Decomposition）是矩阵分解中的一种重要思想，广泛应用于数据压缩、推荐系统、图像处理、机器学习等领域。下面我从几个方面详细介绍这个概念：

### 1 什么是低秩分解？
#### 1.1 矩阵的秩（Rank）
`矩阵的秩` 是指矩阵中线性无关的行或列的最大数目。一个 $m \times n$ 的矩阵 $A$ 的秩最多为 $\min (m, n)$ 。
- 如果一个矩阵的秩很低（远小于 $m$ 和 $n$ ），我们说它是一个＂低秩矩阵＂。

#### 1.2 低秩分解的定义
将一个矩阵 $A \in \mathbb{R}^{m \times n}$ 近似为两个更小矩阵的乘积：

$$
A \approx U V^T
$$


其中：
- $U \in \mathbb{R}^{m \times r}$
- $V \in \mathbb{R}^{n \times r}$
- $r \ll \min (m, n)$ ，称为目标 `秩`

这个分解的意义是将一个大矩阵用两个小矩阵来表示，大大降低了存储和计算成本。

### 2 常见的低秩分解方法
#### 2.1 奇异值分解（SVD）
最经典的低秩分解形式：

$$
A=U \Sigma V^T
$$

- $U \in \mathbb{R}^{m \times m}$ ：左奇异向量
- $\Sigma \in \mathbb{R}^{m \times n}$ ：对角矩阵（奇异值）
- $V \in \mathbb{R}^{n \times n}$ ：右奇异向量

低秩近似做法是取前 $r$ 个最大的奇异值和对应的向量：

$$
A_r=U_r \Sigma_r V_r^T
$$


其中 $A_r$ 是秩为 $r$ 的矩阵，它是最小化 Frobenius 范数误差的最优近似。

#### 2.2 主成分分析（PCA）
PCA 本质上是对数据矩阵做 SVD，然后取前 $r$ 个主成分——这也可以看成一种低秩分解。

#### 2.3 非负矩阵分解（NMF）
约束矩阵的所有元素非负：

$$
A \approx W H, \quad W, H \geq 0
$$


常用于文本挖掘、图像表示等非负数据的降维。

#### 2.4 矩阵补全（Matrix Completion）
比如 Netflix 推荐系统中，用户一电影评分矩阵是稀疏的，低秩分解被用于预测缺失的评分。

$$
\min _{U, V} \sum_{(i, j) \in \Omega}\left(A_{i j}-\left(U V^T\right)_{i j}\right)^2+\lambda\left(\|U\|^2+\|V\|^2\right)
$$


### 3 低秩分解的意义和用途
##### 1. 数据压缩
低秩分解只需保存两个小矩阵，大大减少了存储空间。例如图像压缩。

##### 2. 去噪与缺失值补全
保留主成分可滤掉噪声，同时预测缺失的数据。

##### 3. 推荐系统
如 Netflix 的矩阵补全，用低秩表示用户与物品之间的潜在因子。

##### 4. 特征降维
PCA 和 NMF 都用于将高维数据压缩成低维空间表示，便于建模。


### 4 举个例子（SVD 低秩近似）
假设一个图像矩阵 $A \in \mathbb{R}^{1000 \times 1000}$ ，我们对其做 `SVD`：
- 得到奇异值向量中只有前 `50` 个很大，其他接近 `0`。
- 我们可以只保留前 `50` 个主成分，得到秩为 `50` 的近似矩阵 $A_{50}$
- 原始需要存 $10^6$ 个数，现在只需存 $1000 \times 50+50 \times 50+1000 \times 50 \approx 10^5$ 个数，压缩 10 倍。


### 5 LoRA 的应用

#### 5.1 LoRA 是什么？
LoRA 的核心思想是：在不改变原始模型权重的前提下，只训练一个小的、低秩的权重增量矩阵，从而实现快速且高效的参数微调。

论文出处：LoRA: Low-Rank Adaptation of Large Language Models（https://arxiv.org/abs/2106.09685）



#### 5.2 LoRA 的做法
将原始权重的变化部分 $\Delta W$ 建模为一个低秩矩阵分解形式：

$$
\Delta W=B A, \quad A \in \mathbb{R}^{r \times k}, B \in \mathbb{R}^{d \times r}, \quad r \ll \min (d, k)
$$


于是：

$$
\hat{W}=W+\Delta W=W+B A
$$

- 只训练 $A, B$ ，而不更新 $W$
- $r$ 是一个非常小的秩，比如 `4` 或 `8`
- 这就大大减少了微调参数数量

#### 5.3 LoRA 的优点

| 项目         | 描述                            |
| ---------- | ----------------------------- |
| 🧠 减少训练参数  | 只训练小的 $A, B$，可将参数更新量减少上百倍     |
| 🔧 保留预训练能力 | 不动原始模型权重，保持稳定性                |
| 🧩 易于集成    | LoRA 模块可轻松插入原始 transformer 层中 |
| 📦 多任务适应   | 不同任务可保存不同 $A, B$，快速切换任务       |

>LoRA 与其它微调方法对比

| 方法               | 参数量 | 修改原模型？ | 支持多任务？ |
| ---------------- | --- | ------ | ------ |
| Full fine-tuning | 多   | ✅ 是    | ❌ 不方便  |
| Adapter          | 中等  | ✅ 插入模块 | ✅ 可切换  |
| LoRA             | 少   | ❌ 否    | ✅ 高效切换 |
| Prompt Tuning    | 极少  | ❌ 否    | ✅ 任务特化 |
