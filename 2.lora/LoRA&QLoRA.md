

## 1 LoRA（Low－Rank Adaptation）
>核心思想
- 传统的微调方式需要对整个模型的参数进行更新，而 LoRA 通过在 Transformer 层的某些权重矩阵上引入低秩分解（Low－Rank Decomposition），只调整少量参数，从而减少存储和计算需求。

>工作原理：
- 在权重矩阵 $W$ 上添加一个低秩可训练矩阵 $\Delta W$ ：

    $$
    W^{\prime}=W+\Delta W=W+A B
    $$


    其中：
    - $A$ 和 $B$ 是低秩矩阵（rank＜full rank），可训练；
    - $W$ 是原始模型的权重，保持冻结；
    - $\Delta \mathrm{W}=\mathrm{AB}$ 只有很少的参数可训练，因此减少了计算和存储成本

>优点
- 减少存储开销：只需存储 𝐴 和 𝐵 这两个小矩阵，而不是整个权重矩阵的更新。
- 训练效率高：相比全参数微调，大幅减少显存占用和计算量。
- 适用于多种模型：适用于 GPT、BERT、Qwen 等 Transformer 结构的大模型。

>缺点
- 仍然需要用 `FP16/FP32（全精度）`存储模型权重，导致显存占用较大

## 2 QLoRA（Quantized LoRA）
>核心思想
- 在 LoRA 的基础上进一步引入 `4-bit` 量化（4-bit Quantization），进一步降低显存占用，使得大模型可以在消费级 GPU（如 RTX 3090/4090）上微调。

>工作原理
- 4-bit 量化权重：使用 `NF4（Normal Float 4）`量化技术，将模型主权重矩阵从 `FP16/FP32` 降至 `4-bit` 存储
- 冻结量化权重，仅训练 LoRA 低秩参数（𝐴 和 𝐵）。
- 采用分组量化技术，保证计算效率，减少量化误差对模型精度的影响

>优点
- 极大降低显存需求：LoRA 在 `FP16/FP32` 模型上微调时，`7B` 模型至少需要 `24GB` 显存；而 QLoRA 在 `4-bit` 量化后，仅需 `8GB-16GB` 显存，即可微调 `7B` 模型
- 支持更大模型：可以在消费级显卡上微调 `13B` 甚至 `30B` 级别的大模型。
- 计算效率高：结合 `4-bit` 量化和 LoRA，减少计算复杂度。

>缺点
- 量化误差：由于 4-bit 量化不可避免地会损失一定的信息，对高精度任务可能会有一定影响（但 NF4 量化技术优化了这个问题）
- 依赖特定优化器：如 `bitsandbytes` 库来高效管理 4-bit 量化参数

## 3 LoRA vs. QLoRA 总结对比
特性	|LoRA	|QLoRA
:-|:-|:-
核心方法	|低秩适配	|低秩适配 + 4-bit 量化
显存占用	|高（需要 FP16/FP32 存储模型）	|低（量化后 4-bit 存储）
微调开销	|适中	|极低
支持大模型	|适用于 7B-13B 模型（但高显存需求）	|适用于 7B-30B 甚至 65B 级模型
训练精度	|高	|稍受量化影响，但优化良好
适用场景	|服务器端 GPU（A100、H100）	|消费级 GPU（RTX 3090、4090）

## 4 选用 LoRA 还是 QLoRA？
如果显存充足（A100/H100），且对训练精度要求较高，选择 LoRA。

如果显存有限（如 4090/3090），想在本地/消费级设备上微调，选择 QLoRA。

如果要微调 30B+ 级别的模型，QLoRA 更具优势，因为 4-bit 量化大幅降低显存占用。