## KL 散度

KL 散度在大模型微调里干的事只有一件：
检查“新模型说话的习惯”，和“原来模型说话的习惯”差得有多离谱。

### 场景：模型“学坏了”
>原来的模型（参考模型）最爱说的一句话：
```
中国 | 的 | 首都 | 是 | 北京
```
>微调后的模型（训练模型）学成了这样：
```
中国 | 的 | 北方 | 是 | 上海
```
我们要算的是：新模型这句话，说得有多“不像原来的自己”？

### 关键设定（不抽象）

模型在 每一个位置，都要从一个“词候选池”里选词。
我们只看 出问题的两个位置（其它位置完全一样，惩罚≈0）：
- 第 3 个词
- 第 5 个词

### 第 3 个词位：首都 vs 北方
候选词（简化）

| 词  | 原模型概率 | 新模型概率 |
| -- | ----- | ----- |
| 首都 | 0.9   | 0.1   |
| 北方 | 0.1   | 0.9   |


##### KL 的计算方式
$$
\text { 罚款 }=\text { 新模型概率 } \times \ln \left(\frac{\text { 新模型概率 }}{\text { 原模型概率 }}\right)
$$

##### 对 `首都` 这个词

$$
\begin{gathered}
0.1 \times \ln \left(\frac{0.1}{0.9}\right) \\
=0.1 \times \ln (0.111) \\
=0.1 \times(-2.20)=-0.22
\end{gathered}
$$

##### 对 `北方` 这个词

$$
\begin{gathered}
0.9 \times \ln \left(\frac{0.9}{0.1}\right) \\
=0.9 \times \ln (9) \\
=0.9 \times 2.20=1.98
\end{gathered}
$$

##### 第 3 个词位 KL 小结

$$
K L_3=-0.22+1.98=1.76
$$

👉 非常大，说明这一词位“彻底变性”了


### 第 5 个词位：`北京` vs `上海`
候选词

| 词  | 原模型概率 | 新模型概率 |
| -- | ----- | ----- |
| 北京 | 0.95  | 0.05  |
| 上海 | 0.05  | 0.95  |


#### 对 `北京`

$$
\begin{gathered}
0.05 \times \ln \left(\frac{0.05}{0.95}\right) \\
=0.05 \times \ln (0.0526) \\
=0.05 \times(-2.94)=-0.147
\end{gathered}
$$

#### 对 `上海`

$$
\begin{gathered}
0.95 \times \ln \left(\frac{0.95}{0.05}\right) \\
=0.95 \times \ln (19) \\
=0.95 \times 2.94=2.793
\end{gathered}
$$

第 5 个词位 KL 小结

$$
K L_5=-0.147+2.793=2.646
$$

👉 这是“把常识整个换掉”的级别

### 整句话的 KL（逐词相加）

$$
K L_{\text {总 }}=K L_3+K L_5=1.76+2.646=4.406
$$

### 这个 4.406 是什么概念？

我们对比一个 “稍微改坏一点点”的模型：
```
中国 | 的 | 首都 | 是 | 上海
```
只在最后一个词乱来。

你会得到：
$$
K L \approx 2.646
$$

而你这个模型：

- 词义
- 知识
- 地理

全都一起背叛了

👉 所以 KL 直接翻倍。