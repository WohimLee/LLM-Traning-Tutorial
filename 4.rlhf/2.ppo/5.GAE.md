## GAE 
- 蒙特卡洛法: 方差太大
- 时序差分法: 偏差太大

>广义优势法 

$$
A_t=\sum_{b=0}^{\infty}(\gamma \lambda)^b \delta_{t+b}
$$

GAE 做的事只有一件：
- 把“我这一步到底干得好不好”，
- 不只看眼前这一刻，
- 而是看看接下来几步一起算。

先回忆你已经懂的 δ 是什么

你已经知道：

$$
\delta_t=r_t+\gamma V\left(S_{t+1}\right)-V\left(S_t\right)
$$

一句话：“我这一步，是不是比我原来预期的好？”

#### 为什么光用 δ 不够？

因为在 LLM 里：

- 单个 token 的奖励 噪声很大
- KL 罚分会抖
- reward model 只在句尾给分
👉 某一步 δ 大，可能只是运气好 / 坏

#### GAE 的想法

别只看这一步，
看看“接下来几步”是不是也在证明你刚才那步是对的。

#### 公式先翻译成人话

$$
\boldsymbol{A}_t=\delta_t+(\gamma \lambda) \delta_{t+1}+(\gamma \lambda)^2 \delta_{t+2}+\cdots
$$

意思是：
- 当前 δ：全额算
- 下一步 δ：打个折
- 再下一步：再打折
- 越远，越不重要


### 一、用一个完整、可算的例子（关键）
场景

模型在说一句话：
```
我 | 的 | 老板 | 是 | 土豆
```
我们从 第 3 个词 开始看。

##### 假设 Critic 事后算出的 δ 是：

| 时间步 | δ    |
| --- | ---- |
| δ₃  | +1.0 |
| δ₄  | −0.5 |
| δ₅  | +0.8 |
| δ₆  | −0.2 |

（第 6 步是句子结束）

##### 取常见参数
- $\gamma=1$
- $\lambda=0.9$


##### 开始算 $A_3$

$$
A_3=1.0+0.9 \times(-0.5)+0.9^2 \times 0.8+0.9^3 \times(-0.2)
$$


逐项算：
- 第 1 项： 1.0
- 第 2 项：-0.45
- 第 3 项：+0.648
- 第 4 项：-0.1458

加起来：
$$A_3=1.0522$$


##### 这个 1.05 是什么意思？用人话说：

- “虽然中间有点波动，
- 但从长远看，
- 你第 3 个词这一步是明显走对了。”


##### 对比：如果只用 δ₃ 会怎样？
- 只看 δ₃ = +1.0
- 用 GAE = +1.05

差别不大，说明 后面没强烈反证

### 二、再来一个反例
#### 假设
| 时间步 | δ    |
| --- | ---- |
| δ₃  | +1.0 |
| δ₄  | −2.0 |
| δ₅  | −1.5 |

#### GAE 再算一遍
$$
\begin{gathered}
A_3=1.0+0.9(-2.0)+0.9^2(-1.5) \\
=1.0-1.8-1.215=-2.015
\end{gathered}
$$

#### 这说明什么？

- “你这一步表面上看起来不错，
- 但事实证明：
- 后面一路走崩了。”

👉 所以这一步 不该被强化

### 三、λ 到底在调什么？

这是 GAE 最关键的旋钮。

#### λ = 0

$$
A_t=\delta_t
$$

- 完全短视
- 噪声最大

#### λ → 1

$$
A_t \approx \text { 整句话最终结果 }
$$

- 看得很远
- 方差小
- 但 Critic 不准时会带偏

#### LLM 里的经验值

$$
\lambda \in[0.9,0.97]
$$
- 👉 既看远，又不太信 Critic


### 四、为什么 GAE 特别适合 LLM？

因为：

- 奖励极端稀疏（RM 只在句尾）
- token 数多
- 单步 δ 极不稳定

👉 GAE = 把一句话的“整体感觉”慢慢往前传播

##### 一个非常贴切的比喻

- δ 是：“你刚才那句话是不是说得有点怪？”
- GAE 是：“你这几句话连起来看，到底是在变好，还是在走歪？”

#### 最后一句话总结
GAE = 带记忆的 δ 加权平均
λ 决定你“相信未来多少步”