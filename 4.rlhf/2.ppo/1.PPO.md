## PPO

>æ•´ä½“ä¸€å¥è¯ï¼ˆå…ˆç»™ç»“è®ºï¼‰
- åœ¨ RLHF ä¸­ï¼ŒPPO åšçš„äº‹åªæœ‰ä¸€ä»¶ï¼š
- ç”¨ Advantage è°ƒæ•´æ¯ä¸ª token çš„ç”Ÿæˆæ¦‚ç‡ï¼Œ
- åŒæ—¶é€šè¿‡ KL çº¦æŸä¿è¯æ¨¡å‹ä¸åç¦»å‚è€ƒæ¨¡å‹å¤ªè¿œã€‚

#### Step 1ï¼šActor ç”Ÿæˆ tokenï¼ˆçŠ¶æ€â€“åŠ¨ä½œï¼‰

```
å¤©ç©º ï½œ æ˜¯ ï½œ è”šè“è‰² ï½œ çš„
```
åœ¨ç¬¬ $t$ æ­¥ï¼š
- çŠ¶æ€ $S_t$ ï¼šprompt + å·²ç”Ÿæˆ tokens
- åŠ¨ä½œ $A_t$ ï¼šä¸‹ä¸€ä¸ª token

Actorï¼ˆpolicyï¼‰ç»™å‡ºï¼š

$$
P\left(A_t \mid S_t\right)
$$


åŒæ—¶æˆ‘ä»¬æœ‰ä¸€ä¸ªå†»ç»“çš„ reference policyï¼š

$$
P_{\mathrm{ref}}\left(A_t \mid S_t\right)
$$

#### Step 2ï¼šæ„é€  token-level reward
##### KL å³æ—¶æƒ©ç½šï¼ˆæ¯ä¸€æ­¥éƒ½æœ‰ï¼‰

$$
r_t^{K L}=-k_{k l} \cdot \log \frac{P\left(A_t \mid S_t\right)}{P_{\mathrm{ref}}\left(A_t \mid S_t\right)}
$$


å«ä¹‰ï¼šæ¯ç”Ÿæˆä¸€ä¸ª tokenï¼Œåªè¦åç¦»å‚è€ƒæ¨¡å‹ï¼Œå°±æ‰£åˆ†

##### ç»ˆæ­¢å¥–åŠ±ï¼ˆåªåœ¨æœ€åä¸€æ­¥ï¼‰
åœ¨ $t=T$ ï¼š

$$
r_T=r_T^{K L}+r_{\mathrm{RM}}
$$

ä¹Ÿå°±æ˜¯ï¼š
- KL æƒ©ç½š
- Reward Model å¯¹æ•´å¥çš„åå¥½åˆ†æ•°

è‡³æ­¤ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ï¼š

$$
\left\{r_1, r_2, \ldots, r_T\right\}
$$

#### Step 3ï¼šCritic è¯„ä¼°æ¯ä¸ªçŠ¶æ€

Critic model å¯¹æ¯ä¸ªçŠ¶æ€è¾“å‡ºä¸€ä¸ªå€¼ï¼š

$$
V_\theta\left(S_t\right)
$$

| token | (V(S_t)) |
| ----- | -------- |
| å¤©ç©º    | -0.84    |
| æ˜¯     | 0.88     |
| è”šè“è‰²   | 1.96     |
| çš„     | 3.48     |

å«ä¹‰ï¼šä»å½“å‰ token å¼€å§‹ï¼Œé¢„è®¡æœ€ç»ˆèƒ½æ‹¿åˆ°å¤šå°‘æ€» reward

#### Step 4ï¼šTD è¯¯å·®
å¯¹æ¯ä¸€æ­¥è®¡ç®—ï¼š

$$
\delta_t=r_t+\gamma V(S_{t+1})-V(S_t)
$$
- $r_t$: 
- $V(S_{t+1})$: 
- $V(S_t)$: 


ç›´è§‰è§£é‡Šï¼š
- ï¼‚çœŸå®å‘ç”Ÿçš„ï¼‹ä¸‹ä¸€æ­¥é¢„æœŸï¼‚
- å’Œï¼‚critic åŸæœ¬çš„é¢„æœŸï¼‚å·®å¤šå°‘

è¿™æ˜¯ Advantage çš„æœ€å°ç»„æˆå•ä½ã€‚

#### Step 5ï¼šGAE â†’ Advantage

$$
A_t=\sum_{b=0}^{\infty}(\gamma \lambda)^b \delta_{t+b}
$$


ä½œç”¨ï¼š
- æŠŠåé¢ token çš„å¥½å
- æŒ‰è·ç¦»è¡°å‡
- åˆ†æ‘Šç»™å½“å‰ token

ğŸ“Œ è¿™ä¸€æ­¥æŠŠ token åºåˆ—å¥–åŠ±
å˜æˆäº† token åºåˆ— Advantage

#### Step 6ï¼šPPO æ›´æ–° policyï¼ˆæœ€ç»ˆç›®æ ‡ï¼‰
å®šä¹‰æ¦‚ç‡æ¯”ï¼š

$$
\rho_t=\frac{P\left(A_t \mid S_t\right)}{P_{\text {old }}\left(A_t \mid S_t\right)}
$$


PPO çš„ç­–ç•¥æŸå¤±ï¼ˆTRLï¼OpenAI é£æ ¼ï¼‰ï¼š

$$
C_{\text {clip }}=-\mathbb{E}_t\left[\min \left(\rho_t A_t, \operatorname{clip}\left(\rho_t, 1-\epsilon, 1+\epsilon\right) A_t\right)\right]
$$


å«ä¹‰ï¼š
- å¦‚æœè¿™æ¬¡æ›´æ–°è®© token æ¦‚ç‡å˜å¤ªå¤š
- æ¢¯åº¦ç›´æ¥è¢«æˆªæ–­


### Critic ï¼†Entropyï¼ˆå®Œæ•´ lossï¼‰

$$
\mathcal{L}=C_{\text {clip }}+c_V C_V+C_{\text {ent }}
$$


å…¶ä¸­ï¼š
- $C_V=\frac{1}{2}\left(V\left(S_t\right)-\hat{G}_t\right)^2$
- $C_V$ï¼ˆå¤§å†™ Cï¼‰: Value loss æœ¬èº«,ä¸€ä¸ªâ€œæŸå¤±é¡¹â€ 
- $c_V$ï¼ˆå°å†™ cï¼‰: Value loss çš„æƒé‡ç³»æ•°, ä¸€ä¸ªâ€œè¶…å‚æ•°â€ 
- $C_{\text {ent }}$ ï¼šé˜²æ­¢ç­–ç•¥å¡Œç¼©

#### 1ï¸âƒ£ Value loss æœ¬èº«ï¼š$C_V$

$$
C_V=\frac{1}{2} \mathbb{E}_t\left[\left(V_\theta\left(s_t\right)-\hat{G}_t\right)^2\right]
$$


å«ä¹‰ï¼šcritic é¢„æµ‹çš„ value å’ŒçœŸå® return çš„è¯¯å·®
è¿™æ˜¯ä¸€ä¸ªæ ‡é‡æŸå¤±å‡½æ•°ã€‚

#### 2ï¸âƒ£ æƒé‡ç³»æ•°ï¼š$c_V$
- ä¸€ä¸ªäººä¸ºè®¾å®šçš„è¶…å‚æ•°
- æ§åˆ¶ critic æ›´æ–°åœ¨æ€» loss ä¸­æœ‰å¤šé‡è¦

å¸¸è§å–å€¼ï¼ˆç»éªŒï¼‰ï¼š
- TRLï¼OpenAIï¼š$c_Vï¼ 0.5$
- æœ‰æ—¶ï¼š $0.1 \sim 1.0$