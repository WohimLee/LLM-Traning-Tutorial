## PPO 的 KL 奖励（即时）

$$
r_t^{K L}=-k_{k l} \cdot \log \frac{P\left(A_t \mid S_t\right)}{P_{\mathrm{ref}}\left(A_t \mid S_t\right)}
$$



>例子
```
# 原来的模型：
我 | 的 | 宠物 | 是 | 猫咪

# 训练后的模型：
我 | 的 | 老板 | 是 | 土豆
```

### 一、两个模型在“同一时刻”的想法
原模型（参考模型）心里想的是：

| 词  | 概率  |
| -- | --- |
| 宠物 | 0.9 |
| 老板 | 0.1 |

新模型（PPO 正在训练的）：

| 词  | 概率  |
| -- | --- |
| 宠物 | 0.1 |
| 老板 | 0.9 |

新模型明显开始放飞自我了。

### 二、PPO 不想等你一句话说完再算账

这是关键点：PPO 是“边说边罚”，不是“秋后算总账”

所以它只看：

$$
\log \frac{P_{\text {新 }}(\text { 你刚说的词 })}{P_{\text {原 }}(\text { 这个词 })}
$$

### 三、这个 log 比值到底在干嘛？



#### ✅ 情况 1：你说了一个“和原来差不多”的词

$$
P  =0.4, \quad P_{\mathrm{ref}}=0.5
$$

$$
\log \frac{0.4}{0.5}=\log (0.8)  \approx-0.22
$$

再乘一个负号：
$$
r_t^{K L} \approx+0.22 k
$$
👉 轻微鼓励（基本没事）

#### ⚠️ 情况 2：你开始明显偏离

$$
\begin{gathered}
P=0.9, \quad P_{\text {ref }}=0.1 \\
\log \frac{0.9}{0.1}=\log (9) \approx 2.2 \\
r_t^{K L}=-2.2 k
\end{gathered}
$$

👉 奖励很小（负数了，一次重罚）

#### ❌ 情况 3：你在原模型几乎不会说的地方乱说
$$
\begin{gathered}
P_{\mathrm{ref}} \approx 0 \\
\log \frac{P}{0} \rightarrow \infty \\
r_t^{K L} \rightarrow-\infty
\end{gathered}
$$

👉 直接“红牌罚下”

### 六、为什么是「log 概率比」而不是别的？

这是 PPO 里一个非常工程化的选择：

#### 1️⃣ log 可以直接加

一句话的总 KL 惩罚：

$$
\sum_t \log \frac{P\left(A_t \mid S_t\right)}{P_{\text {ref }}\left(A_t \mid S_t\right)}
$$

= 整句话的 KL

不用存整分布，逐词就能算。

#### 2️⃣ log 比值 = “你比原来狂了几倍”

- 比原模型大 2 倍 → log ≈ 0.69
- 大 10 倍 → log ≈ 2.3
- 大 100 倍 → log ≈ 4.6

👉 惩罚增长是平滑但迅速的

#### 3️⃣ 方向是 PPO 想要的那一边

注意这里是：

$$
\log \frac{P_{\text {新 }}}{P_{\text {ref }}}
$$

意思是：只关心你“自己更想说”的地方

这正好对应：

- 奖励 hacking
- 胡编乱造
- 模式崩坏

### 七、那它和“真正的 KL”是什么关系？

你之前算过完整 KL：

$$
K L\left(P_{\text {新 }} \| P_{\text {ref }}\right)=\sum P_{\text {新 }} \log \frac{P_{\text {新 }}}{P_{\text {ref }}}
$$

而 PPO 做的是：
- 采样了一个词
- 用这个词的 $\log \frac{P}{P_{\text {ref }}}$ 当即时罚款

👉 这是 KL 的“单样本版本”

从统计上说：多次采样的平均值 ≈ KL

### 八、为什么前面要加一个负号？

因为 PPO 在最大化奖励。

而 KL 本质上是：“偏离原模型的代价”

所以必须：
$$
\text { 奖励 }= -\text {代价 }
$$


