## DPO

一句话：让模型更符合人类偏好，而不使用强化学习

更具体一点：
- 给定同一个 prompt
- 人类告诉你：A 比 B 好
- 希望模型学会：更倾向生成 A，而不是 B

### 一、DPO 用的是什么数据？

DPO 只需要一种数据格式：
```
(prompt, chosen_response, rejected_response)
```

例如：

- prompt：天空是什么颜色的？
- chosen：天空是蔚蓝色的，因为光的散射
- rejected：天空是蔚蓝色的

⚠️ 注意：

- 不要求“绝对正确”
- 只要求“相对更好”

### 二、DPO 的核心设计（非常重要）

DPO 有 三个关键设定：

#### 1️⃣ 两个模型，而不是一个
##### Policy Model（训练中）

- 要被更新
- 目标：更偏向 chosen

##### Reference Model（冻结）

- 不参与训练
- 通常是 SFT 后的模型
- 作用：防止模型学歪
👉 Reference Model 是 DPO 稳定性的核心

#### 2️⃣ 不生成，只算概率

DPO 不做 rollout、不采样、不生成文本。

它做的只有一件事：计算“已有回答序列”的概率

也就是：
- 直接把 chosen / rejected 的 token
- 丢进模型
- 拿 log probability
- 对 token 求和

#### 3️⃣ 学的是“相对偏好”，不是“绝对奖励”

DPO 从不问：这个回答值多少分？

它只关心：
- 在同一个 prompt 下，
chosen 的概率 是否比 rejected 大？

### 三、DPO 到底在优化什么？（直觉理解）

对每一条偏好数据，DPO 希望做到：

##### 1 Policy 模型 比 Reference

- 更喜欢 chosen
- 更不喜欢 rejected

##### 2 但 整体行为不要偏离 Reference 太多

于是目标就变成：

在 Reference 的基础上，把概率往人类偏好方向推一点点

### 四、DPO 损失在干什么（不看公式版）

可以把 DPO loss 理解成一个问题：

“在这个 prompt 下，Policy 是否比 Reference
更明显地区分了好回答和坏回答？”

- 是 → loss 小（奖励）
- 否 → loss 大（惩罚）

β（beta）控制一句话的语气：
- β 小：温和调整
- β 大：激进拉开差距

### 五、训练流程（完整但简洁）

DPO 训练 = 一次普通的监督微调

流程如下：
- 准备偏好数据
- 拷贝一份模型作为 Reference（冻结）
- 用 DPO loss 微调 Policy
- 训练结束

✔️ 没有：
- 奖励模型
- PPO
- 强化学习循环
- 在线采样

### 七、DPO 和 RLHF 的核心差别

| 维度    | DPO | RLHF  |
| ----- | --- | ----- |
| 奖励模型  | 不需要 | 需要    |
| 强化学习  | 不需要 | 需要    |
| 训练稳定性 | 高   | 容易不稳定 |
| 工程复杂度 | 低   | 高     |
| 可复现性  | 强   | 弱     |

一句话结论：DPO 是“把 RLHF 的目标，改写成一个监督学习损失”