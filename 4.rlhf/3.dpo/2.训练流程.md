## 训练流程

### 一、提前准备好的偏好数据（Preference Data）

成对偏好数据：

- 同一个 prompt：
    ```
    「天空是什么颜色的」
    ```
- 被选择（chosen / y⁺）：
    ```
    「天空 是 蔚蓝色 的 ，因为 太阳 光 的 散射」
    ```
- 被拒绝（rejected / y⁻）：
    ```
    「天空 是 蔚蓝色 的」
    ```
这正是 DPO 所需要的最小数据单元：
$$
\left(x, y^{+}, y^{-}\right)
$$

>✅ 关键点：
- 不需要“正确答案”
- 只需要“人更喜欢哪个回答”

### 二、准备 2 个模型
#### 1️⃣ Policy Model（要训练的模型）

- 黄色方块
- 参数 可更新
- 目标：更偏向 y⁺，远离 y⁻

#### 2️⃣ Reference Model（参考模型）

- 蓝色方块
- 参数冻结
- 一般就是 SFT 后的模型拷贝
- 作用：👉 约束 Policy Model 不要偏离原模型太远

这一步就是 DPO “不需要奖励模型，但需要参考模型” 的来源

### 三、计算 DPO 损失（核心）
#### ① 不做生成（这是重点）

图中写得很清楚：

- ❌ 不需要让模型真的生成完整回答
- ✅ 只需要计算 已有 token 序列的概率

也就是：
- 把 y⁺、y⁻ 的 token
- 直接喂给两个模型
- 取每个 token 的 log probability

#### ② 分别计算 4 个概率

图右侧展示的是：

| 模型        | 好回答 y⁺      | 坏回答 y⁻      |
| --------- | ----------- | ----------- |
| Reference | π_ref(y⁺|x) | π_ref(y⁻|x) |
| Policy    | π_θ(y⁺|x)   | π_θ(y⁻|x)   |

这些数值是：
- 逐 token 概率
- 实际实现中是 logits → log_softmax → sum

#### ③ 带参考模型的偏好对比（DPO 本质）

最终用的就是图中这条公式：
$$
\mathcal{L}_{\mathrm{DPO}}\left(\pi_\theta ; \pi_{\mathrm{ref}}\right)=-\mathbb{E}\left[\log \sigma\left(\beta\left(\log \frac{\pi_\theta\left(y^{+} \mid x\right)}{\pi_{\mathrm{ref}}\left(y^{+} \mid x\right)}-\log \frac{\pi_\theta\left(y^{-} \mid x\right)}{\pi_{\mathrm{ref}}\left(y^{-} \mid x\right)}\right)\right)\right]
$$

直觉解释一句话版：如果 Policy Model 相比 Reference
- 更偏向 y⁺，更不偏向 y⁻
- 👉 那就奖励它 👉 否则就惩罚它